{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Fat Level Of Cheese Types\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In the following we will try to find the best way to predict fat level of different cheese types using their properties. This is an important question to be answered as customers make their decisions according fat level, and when introducing new products, this factor should be considered before making decisions on milk type, processing, etc., as they all influence the fat level. \n",
    "\n",
    "This is a classification problem since the fat level is reported as high and low and not the exact percentage. This means that our attempt will be on recognizing different classes. It is also noteworthy that there are only two classes and this is a binary problem.\n",
    "\n",
    "The positive class is the higher fat group as recognizing these products is of higher importance, because missing them is undesirable.  \n",
    "\n",
    "\n",
    "## Exploratory Data Analasys\n",
    "\n",
    "A variety of packages and functions will be used in this work and they're imported here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "import scipy\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import altair as alt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import lognorm, loguniform, randint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in cheese_data.csv and has been obtained from Kaggle and follows an Open Government Licence (Canada). The original data was found on the Government of Canada's Open Government Portal but has unfortunately been taken down. However, the dataused here, has been modified, wrangled and cleaned by UBC extended learning machine learning team.\n",
    "In the following, the data is read and saved as cheese_df, and is studied briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_df = pd.read_csv('cheese_data.csv')\n",
    "\n",
    "cheese_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to getting a better understanding of the features, it is useful to study their unique values. Later, these results will be used to group features and preprocess them and handle missing values.\n",
    "To keep the result, text columns have been omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['ManufacturerProvCode', 'ManufacturingTypeEn', 'MoisturePercent', 'Organic', 'CategoryTypeEn', 'MilkTypeEn', 'MilkTreatmentTypeEn', 'FatLevel']:\n",
    "    print(col,cheese_df[col].unique(),cheese_df[col].dtype\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FatLevel column which is the target, has categorical values. Since our algorithms work with numbers, we implement this change to have 1 for higher and 0 for lower fat level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_df['FatLevel']=cheese_df['FatLevel'].replace({'lower fat': 0, 'higher fat': 1})\n",
    "cheese_df['FatLevel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the data to train and test subset and then separate our feature vectors from the target. The FatLevel column is used as the target y, and other columns are used as features. This should be done for both train_df and test_df and the results is saved in objects named X_train, y_train, X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df = train_test_split(cheese_df,test_size=0.2,random_state=123)\n",
    "X_train,y_train = train_df.drop(columns=['FatLevel']),train_df['FatLevel']\n",
    "X_test,y_test = test_df.drop(columns=['FatLevel']),test_df['FatLevel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts are valueable tools to evaluate our features, their overall relation to the target and distributions. \n",
    "In the folloing two charts are presented. The first one illustrates the distribution of target values, and in the second figure, the relation between moisture and fat level in different cheese types are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = alt.Chart(cheese_df).mark_bar().encode(x='FatLevel',y=alt.Y('count()',title='number of samples in each fat level group')).properties(title='distribution of values in target column (FatLevel)')\n",
    "target_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the target class is not balanced but the positive class is not very rare. To be more precise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_df['FatLevel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moisture_Fat= alt.Chart(cheese_df).mark_point().encode(x='MoisturePercent',y='FatLevel')\n",
    "Moisture_Fat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that moisture range in lower fat class, lies in larger amounts and it seems that higher moisture contriutes to lowering the fat level. Some key values can be noteworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_df.groupby(by='FatLevel')['MoisturePercent'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, features are groupped and for each group a transformer is provided to handle missing values, and either implement scaling or OneHotEncoding depending on the nature of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['MoisturePercent']\n",
    "categorical_features = ['ManufacturerProvCode','ManufacturingTypeEn','CategoryTypeEn','MilkTypeEn','MilkTreatmentTypeEn']\n",
    "binary_features = ['Organic']\n",
    "ordinal_features = []\n",
    "drop_features = ['CheeseId','RindTypeEn']\n",
    "text_features = ['FlavourEn','CharacteristicsEn','CheeseName']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(dtype=int, handle_unknown=\"ignore\"))\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='most_frequent'),\n",
    "    OneHotEncoder(dtype=int, handle_unknown=\"error\",drop='if_binary'))\n",
    "\n",
    "text_transformer = make_pipeline(CountVectorizer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods And Results\n",
    "In the following we define a column transformer using make_column_transformer called preprocessor for the numerical, categorical, and remainding feature types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer,numeric_features),\n",
    "    (categorical_transformer,categorical_features),\n",
    "    (binary_transformer,binary_features),\n",
    "    (\"drop\",drop_features),\n",
    "    (\"drop\",text_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pipelines we can apply our preproccers and classifiers. In this report four models, DummyClassifier as a baseline model, KNeighborsClassifier and SVC as logistic models, and RandomForestClassifier as an ensemble of decisiontrees, are used. \n",
    "As it was shown, the target is unbalanced. For now, we continue with the unbalanced data. Later with weightings, we will address this problem and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ub_dum=make_pipeline(preprocessor,DummyClassifier())\n",
    "pipe_ub_knn=make_pipeline(preprocessor,KNeighborsClassifier())\n",
    "pipe_ub_svc=make_pipeline(preprocessor,SVC())\n",
    "pipe_ub_rfc=make_pipeline(preprocessor,RandomForestClassifier(random_state=77))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, a scoring dictionary is provided in order to examine our models with different metrics. This will be used in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_meth={'accuracy':'accuracy','precision':'precision','recall':'recall','f1':'f1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, using cross-validations on X_train and y_train using our pipelines, we can compare our models' performances. Averages of all results are reported to ease the comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ub_dum=pd.DataFrame(cross_validate(pipe_ub_dum,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_ub_dum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately Dummy model fails to predict any example from the positive class and returns 0 for recall, precission and f1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ub_knn=pd.DataFrame(cross_validate(pipe_ub_knn,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_ub_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and validation accuracy for KNN are 0.8634 and 0.8103 respectively. However, accuracy is not the best metrics for assessing a model and other scores can present a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ub_svc=pd.DataFrame(cross_validate(pipe_ub_svc,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_ub_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores for SVM are slightly lower than KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ub_rfc=pd.DataFrame(cross_validate(pipe_ub_rfc,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_ub_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scores have improved dramatically with RandomForestClassifier. However, the bigger gap between training and test results can be a sign of overfitting. \n",
    "\n",
    "Another problem to address is the imbalance in the data. Therefore, a new pipeline with RandomForestClassifier with class_weight=\"balanced\" is introduced and similar workflow is followed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_b_rfc=make_pipeline(preprocessor,RandomForestClassifier(class_weight=\"balanced\",random_state=77))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_b_rfc=pd.DataFrame(cross_validate(pipe_b_rfc,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_b_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes are not drastic. This can be because of the fact that the imbalance is not huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to find the best parameters, a RandomizedSearchCV named random_search over a grid of two hyperparameters under the name param_dist is carried out. These hyperparameters are n_estimators and max_depth and change over a range of 10 to 300 and 2 to 20 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"randomforestclassifier__n_estimators\": scipy.stats.randint(low=10, high=300),\n",
    "    \"randomforestclassifier__max_depth\": scipy.stats.randint(low=2, high=20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV (\n",
    "    pipe_b_rfc,\n",
    "    param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring = \"f1\",\n",
    "    random_state=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that random_search is fitted on X_train and y_train, we can extract the results. In the following, best parameters and model, and optimal scores are calculated and reported. These scores can be compared with the amounts obtained with default values for hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_parameters = random_search.best_params_\n",
    "optimal_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = random_search.best_estimator_\n",
    "optimal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can atain using score function are the accuracies, which have even dropped. However, other scores can be calculated to show the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_score = random_search.score(X_train,y_train)\n",
    "training_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_score = random_search.score(X_test,y_test)\n",
    "testing_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_recal=round(recall_score(y_test,random_search.predict(X_test)),2)\n",
    "optimal_recal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_precision=round(precision_score(y_test,random_search.predict(X_test),average='weighted'),2)\n",
    "optimal_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_f1=round(f1_score(y_test,random_search.predict(X_test),average='weighted'),2)\n",
    "optimal_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scores, except for the accuracy, have increased considerably. \n",
    "Results can be presented more clearly using classification_report and plot_confusion_matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,random_search.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot = plot_confusion_matrix(random_search,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "according the confusion matrix, 118 Negatives and 58 Positives were correctly recognised and the model has failed to predict 18 Negatives and 15 Positives. Although this is a satisfying result, missing 15 Positives from just 73 is not a good sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['MoisturePercent']\n",
    "categorical_features = ['ManufacturerProvCode','ManufacturingTypeEn','CategoryTypeEn','MilkTypeEn','MilkTreatmentTypeEn']\n",
    "binary_features = ['Organic']\n",
    "ordinal_features = []\n",
    "drop_features = ['FlavourEn','CheeseId','RindTypeEn','CheeseName']\n",
    "text_features = ['CharacteristicsEn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_text = make_column_transformer(\n",
    "    (numeric_transformer,numeric_features),\n",
    "    (categorical_transformer,categorical_features),\n",
    "    (binary_transformer,binary_features),\n",
    "    (\"drop\",drop_features),\n",
    "    (text_transformer,text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_LR=make_pipeline(preprocessor_text,LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_LR=pd.DataFrame(cross_validate(pipe_LR,X_train,y_train,cv=5,return_train_score=True,scoring=scoring_meth)).mean()\n",
    "scores_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"logisticregression__C\": loguniform(0.01, 100),\n",
    "    \"countvectorizer__max_features\": randint(1, 10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_text=RandomizedSearchCV(pipe_LR,param_grid,n_iter=10,cv=5,random_state=888,n_jobs=-1,verbose=3,return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_text.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen earlier, hyperparameter optimizing improves all scores but accuracy. Actually the data is not balanced so accuracy based optimization leads to a worse prediction. Hence, other methods are of importance. \n",
    "\n",
    "In this report, text features are not studied. Obviously they can contribute to making better predictions.\n",
    "\n",
    "Another interesting question is finding major patterns such as the distribution of FatLevel in different MilkTypeEn subsets for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refrences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle \n",
    "\n",
    "Open Government Licence (Canada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "973cda5a46c9e738ec2f0ac3ed2d1fb7b1ec715167bd81706e11b8a8f84d9631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
